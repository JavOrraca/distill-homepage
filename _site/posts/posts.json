[
  {
    "path": "posts/2021-03-11-rayshader/",
    "title": "Beautiful Maps with Rayshader",
    "description": "If you love beautiful 2D and 3D maps, now you can create your own with  elevation data, the rayshader package, OpenStreetMap, and ggplot2.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [
      "rayshader",
      "raster",
      "osmdata",
      "sf",
      "ggplot2"
    ],
    "contents": "\n\nContents\nOverview\nSetup\nLibraries\nEDA\nCropping\n\n2D Map\nBase Map\nOpenStreetMap\nLayering\nFinal Touches\n\n3D Map\nFeatures\nNext Steps\nSession Info\n\n\nOverview\nR, rayshader, open street maps‚Ä¶ Lots of amazing learning to be had with these geomapping tools. I‚Äôm following Tyler Morgan-Wall‚Äôs _Adding Open Street Map Data to Rayshader Maps in R tutorial and learning a lot about the development of beautiful maps with R. Laguna Beach is one of my favorite cities in the USA - and in the world - so I applied the concepts from this tutorial to Laguna Beach My work-in-progress code and screen captures are below.\nTyler Morgan-Wall‚Äôs introduction to the rayshader package and step-by-step tutorial make the process easy to follow and when you get stuck, you can almost always find an answer on his rayshader website (excellent technical documentation and code examples). One last note before I dive into my code chunks below - Finding high-quality LIDAR or Digital Elevation Models (DEMs) for United States cities was not easy. To reproduce the steps below for your location of choice, you may have to get creative with cropping LIDAR/DEMs to box the geography that you actually want to model, as I had to do with my example below.\nSetup\nIf you‚Äôre only wanting to learn, the team at shadedrelief.com has several publicly available high-resolution DEMs for you to choose from: http://shadedrelief.com/SampleElevationModels/\nFor the purposes of reproducibility, feel free to download the LIDAR (TIF format) of Laguna Canyon, California, that was my starting point. This data, made available by the United States Geological Survey‚Äôs National Geologic Map Database, was captured in 2018 as part of a study on Southern California wildfires. The geo-coordinate data points include latitude, longitude, and elevation, and are available roughly one meter apart (hence the 257 MB filesize). You can download this TIF from my Google Drive here: https://drive.google.com/Javier/USGS_TIF\nStart by installing the development versions of the rayshader and rayimage packages, and loading several others packages used in this project. If you‚Äôre using a Linux machine, you may need to install additional system libraries to properly render these visuals.\nLibraries\n\n\n# Install dev packages\ndevtools::install_github(\"tylermorganwall/rayshader\")\ndevtools::install_github(\"tylermorganwall/rayimage\")\n\n# Load libraries\nlibrary(rayshader)\nlibrary(raster)\nlibrary(osmdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# Source elevation data\nlaguna <- raster::raster(\"./USGS_one_meter_x42y372_CA_SoCal_Wildfires_B1_2018.tif\")\n\n\n\nEDA\nNext, we can visualize the elevation data with rayshader defaults to understand general elevation differences in the data set. Before we can render this visual, we need to transform the laguna data structure to a matrix. For faster rendering, we can also resize the matrix using rayshader::resize_matrix() that can downscale using bilinear interpolation.\n\n\n# Transform the spatial data structure into a regular R matrix\nlaguna_mat <- rayshader::raster_to_matrix(laguna)\n\n# Create small version of matrix for quick visual prototyping\nlaguna_small <- rayshader::resize_matrix(laguna_mat, 0.1)\n\n# Plot base map with using basic color defaults\nlaguna_small %>% \n  rayshader::height_shade() %>% \n  rayshader::plot_map()\n\n\n\nUSGS Laguna Canyon LIDARCropping\nI wanted to focus only on downtown Laguna Beach. This only represents a fraction of the raw LIDAR. To get the lat-long range of my desired geography, I used Google Maps to obtain the endpoints of my desired box. Below, we use a crazy-looking function (thank you Tyler Morgan-Wall) to accomplish this geospatial cropping:\n\n\n# Crop to fit desired box around downtown Laguna Beach\nlat_range <- c(33.541370925562376, 33.55945552155357)\nlong_range <- c(-117.79622401045448, -117.77493476466955)\n\nconvert_coords <- function(lat, long, from = CRS(\"+init=epsg:4326\"), to) {\n  data = data.frame(long = long, lat = lat)\n  sp::coordinates(data) <- ~ long + lat\n  sp::proj4string(data) = from\n  # Convert to coordinate system specified by EPSG code\n  xy = data.frame(sp::spTransform(data, to))\n  colnames(xy) = c(\"x\",\"y\")\n  return(unlist(xy))\n}\n\nraster::crs(laguna)\n\nutm_bbox <- convert_coords(lat = lat_range,\n                           long = long_range,\n                           to = crs(laguna))\n\nraster::extent(laguna)\n\nextent_zoomed <- raster::extent(utm_bbox[1], utm_bbox[2], \n                                utm_bbox[3], utm_bbox[4])\nlaguna_zoom <- raster::crop(laguna, extent_zoomed)\nlaguna_zoom_mat <- rayshader::raster_to_matrix(laguna_zoom)\n\n\n\nBase Map2D Map\nBase Map\nI was inspired by the color schemes that Tyler Morgan-Wall shared, specifically the blueish-pinkish palette made popular by Swiss cartographer‚Äôs Eduard Imhof with his relief shading work. Try adjusting the options below before moving forward as we will continue to build off of this initial base map layer.\n\n\nbase_map <- laguna_zoom_mat %>% \n  rayshader::height_shade() %>% \n  rayshader::add_overlay(\n    rayshader::sphere_shade(\n      laguna_zoom_mat, \n      texture = rayshader::create_texture(\"#f5dfca\",\"#63372c\",\"#dfa283\",\"#195f67\",\"#c2d1cf\",\n                                          cornercolors = c(\"#ffc500\", \"#387642\", \"#d27441\",\"#296176\")),\n      sunangle = 0, \n      colorintensity = 5)) %>%\n  rayshader::add_shadow(rayshader::lamb_shade(laguna_zoom_mat), 0.2) %>%\n  rayshader::add_overlay(\n    rayshader::generate_altitude_overlay(\n      rayshader::height_shade(laguna_zoom_mat, texture = \"#91aaba\"),\n      laguna_zoom_mat,\n      start_transition = min(laguna_zoom_mat)-200,\n      end_transition = max(laguna_zoom_mat)))\n\nrayshader::plot_map(base_map)\n\n\n\nOpenStreetMap\nOpenStreetMap (OSM) is a collaborative project to create a free editable geographic database of the world and the underlying geodata includes details for roads, buildings, hiking paths, rivers, etc. We‚Äôll leverage the osmdata package to import OSM data into R from the overpass API. The osmdata processes routines in C++ for fast construction and loading into R. We‚Äôll load several layers of OSM details, followed by converting those sf objects to the coordinate system we‚Äôre working with for this rayshader project.\n\n\nosm_bbox = c(long_range[1],lat_range[1], long_range[2],lat_range[2])\n\nlaguna_highway <- osmdata::opq(osm_bbox) %>% \n  osmdata::add_osm_feature(\"highway\") %>% \n  osmdata::osmdata_sf() \n\n# Transform coordinates to new projection\nlaguna_lines <- sf::st_transform(laguna_highway$osm_lines,\n                                 crs = raster::crs(laguna))\n\n# View streets as a ggplot2 render \nggplot(laguna_lines, aes(color = osm_id)) + \n  geom_sf() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Open Street Map `highway` attribute in Laguna Beach\")\n\n\n\nOSM render using ggplot2Layering\nTo begin, lets simply layer all the ‚Äúhighway‚Äù OSM attributes on top of our base map using white lines to represent the lines.\n\n\n# Transform sf LINESTRING geometry and create semi-transparent overlay\nbase_map %>% \n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_lines, extent = extent_zoomed,\n      linewidth = 3, color = \"white\",\n      heightmap = laguna_zoom_mat)) %>% \n  rayshader::plot_map()\n\n\n\nBase Map Layer 1I want different styles for different OSM lines. To accomplish this, we‚Äôll create several groups of OSM lines and begin prototyping with styles.\n\n\n# Subset layers of laguna_lines\nlaguna_trails <- laguna_lines %>% \n  dplyr::filter(highway %in% c(\"path\", \"bridleway\", \"steps\", \"track\"))\n\nlaguna_footpaths <- laguna_lines %>% \n  dplyr::filter(highway %in% c(\"footway\"))\n\nlaguna_roads <- laguna_lines %>% \n  dplyr::filter(highway %in% c(\"unclassified\", \"primary\", \n                               \"primary_link\", \"secondary\", \n                               \"tertiary\", \"residential\", \"service\"))\n\n# Create one encompassing layer with independent element styling\ntrails_layer <- \n  rayshader::generate_line_overlay(\n    \n    # Pink footpaths\n    laguna_footpaths, extent = extent_zoomed,\n    linewidth = 4, color = \"pink\",\n    heightmap = laguna_zoom_mat) %>%\n  \n  # Note: While barely visible in the final outputs, the following\n  # adds what looks like a black shadow to the white dashed lines\n  # representing the hiking trails around Laguna Beach \n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_trails, extent = extent_zoomed,\n      linewidth = 4, color = \"black\", lty = 3, offset = c(2,-2),\n      heightmap = laguna_zoom_mat)) %>%\n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_trails, extent = extent_zoomed,\n      linewidth = 4, color = \"white\", lty = 3,\n      heightmap = laguna_zoom_mat)) %>%\n  \n  # White roads\n  rayshader::add_overlay(\n    rayshader::generate_line_overlay(\n      laguna_roads, extent = extent_zoomed,\n      linewidth = 3, color = \"white\",\n      heightmap = laguna_zoom_mat))\n\n\n\nWe‚Äôll create some additional projections of OSM layers (initially done to pull in building data, but this is a WIP).\n\n\nlaguna_parking <- osmdata::opq(osm_bbox) %>% \n  osmdata::add_osm_feature(\"parking\") %>% \n  osmdata::osmdata_sf() \n\nlaguna_building <- osmdata::opq(osm_bbox) %>% \n  osmdata::add_osm_feature(\"building\") %>% \n  osmdata::osmdata_sf()\n\nlaguna_parking_poly <- st_transform(laguna_parking$osm_polygons, crs = crs(laguna))\nlaguna_building_poly <- st_transform(laguna_building$osm_polygons, crs = crs(laguna))\n\n\n\nFinal Touches\nNow we‚Äôre ready to build upon our base map and see the 2D visual in action. We‚Äôll begin by creating a polygon layer of the parking and building features, and adding the styled trails_layer OSM lines on top of that.\n\n\npolygon_layer <- \n  rayshader::generate_polygon_overlay(laguna_parking_poly, \n                                      extent = extent_zoomed,\n                                      heightmap = laguna_zoom_mat, \n                                      palette = \"grey30\") %>%\n  rayshader::add_overlay(\n    rayshader::generate_polygon_overlay(laguna_building_poly, \n                                        extent = extent_zoomed,\n                                        heightmap = laguna_zoom_mat, \n                                        palette = \"lightgrey\"))\n\nbase_map %>% \n  rayshader::add_overlay(polygon_layer) %>%\n  rayshader::add_overlay(trails_layer) %>%\n  rayshader::plot_map()\n\n\n\n2D render of Laguna Beach3D Map\nFeatures\nTo render a 3D map, we follow a very similar pipeline to the 2D map but utilizing rayshader::plot_3d(). This function introduces new arguments and I spent more time than I anticipated adjusting the zscale, zoom, rotation, pitch, and water specifications.\nTo exaggerate the elevation details in this area, I decided on a zscale < 1 that would still resemble the reality and beauty of the Laguna Beach hills - Something too low, e.g., zscale = 0.25, was too exaggerated for my liking. Once the 3D viewer renders your visual, enter rayshader::render_snapshot() into your console to take a flat snapshot of your 3D rendering, saving your snapshot if desired.\n\n\nbase_map %>% \n  rayshader::add_overlay(polygon_layer) %>%\n  rayshader::add_overlay(trails_layer) %>%\n  rayshader::plot_3d(heightmap = laguna_zoom_mat, zscale = 0.75, fov = 0, theta = -45, zoom = 0.75, phi = 45,\n                     water = TRUE, waterdepth = 4, wateralpha = 0.4, watercolor = \"lightblue\",\n                     waterlinecolor = \"white\", waterlinealpha = 0.5,\n                     background = \"white\")\n\n# Create 2D snapshot of 3D rendering\nrayshader::render_snapshot()\n\n\n\n3D render of Laguna BeachNext Steps\nI haven‚Äôt revisited this project since Mar 2021 (finalizing this post now in Sep 2021), but I‚Äôve seen some neat examples where developers added 3D polygons representing the businesses and residential housing, and additional styling for higher-resolution quality. Additionally, creating a 3D flyover video with the rayshader package looks phenomenal. For now, I hope this was helpful. Tyler Morgan-Wall knocked it out of the park with the package and his tutorials, so I hope you find his content as useful as I did.\nSession Info\n\nR version 4.1.1 (2021-08-10)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.9.0\nLAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.9.0\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ggplot2_3.3.5    dplyr_1.0.7      sf_1.0-2         osmdata_0.1.6   \n[5] raster_3.4-13    sp_1.4-5         rayshader_0.26.1\n\nloaded via a namespace (and not attached):\n [1] rgl_0.107.14       Rcpp_1.0.7         lubridate_1.7.10   lattice_0.20-44   \n [5] png_0.1-7          prettyunits_1.1.1  class_7.3-19       ps_1.6.0          \n [9] assertthat_0.2.1   rprojroot_2.0.2    digest_0.6.27      foreach_1.5.1     \n[13] utf8_1.2.2         R6_2.5.1           e1071_1.7-8        httr_1.4.2        \n[17] pillar_1.6.2       rlang_0.4.11       progress_1.2.2     curl_4.3.2        \n[21] callr_3.7.0        magick_2.7.3       pkgdown_1.6.1      desc_1.3.0        \n[25] devtools_2.4.2     rgdal_1.5-23       htmlwidgets_1.5.3  munsell_0.5.0     \n[29] proxy_0.4-26       compiler_4.1.1     xfun_0.25          pkgconfig_2.0.3   \n[33] pkgbuild_1.2.0     htmltools_0.5.2    tidyselect_1.1.1   tibble_3.1.4      \n[37] codetools_0.2-18   fansi_0.5.0        crayon_1.4.1       withr_2.4.2       \n[41] grid_4.1.1         gtable_0.3.0       jsonlite_1.7.2     lifecycle_1.0.0   \n[45] DBI_1.1.1          magrittr_2.0.1     scales_1.1.1       units_0.7-2       \n[49] KernSmooth_2.23-20 cli_3.0.1          cachem_1.0.6       farver_2.1.0      \n[53] fs_1.5.0           remotes_2.4.0      doParallel_1.0.16  testthat_3.0.4    \n[57] xml2_1.3.2         ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8       \n[61] iterators_1.0.13   tools_4.1.1        rayimage_0.6.2     glue_1.4.2        \n[65] purrr_0.3.4        hms_1.1.0          processx_3.5.2     pkgload_1.2.1     \n[69] parallel_4.1.1     fastmap_1.1.0      colorspace_2.0-2   sessioninfo_1.1.1 \n[73] classInt_0.4-3     rvest_1.0.1        memoise_2.0.0      knitr_1.33        \n[77] usethis_2.0.1     \n\n\n\n\n",
    "preview": "posts/2021-03-11-rayshader/images/3D_render.png",
    "last_modified": "2021-09-06T16:08:07-07:00",
    "input_file": {},
    "preview_width": 619,
    "preview_height": 574
  },
  {
    "path": "posts/2021-01-24-rstudio-global/",
    "title": "rstudio::global tips, tricks, and more",
    "description": "RStudio's annual conference saw roughly 17,000 attendees for  their first global, all-virtual, 24-hour event. I attended several sessions throughout the day and I'll highlight my favorite data bytes learned that day. I'll also share relatable content for better modeling with R.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2021-01-24",
    "categories": [
      "RStudio",
      "Conference",
      "tidymodels",
      "R"
    ],
    "contents": "\n\nContents\nFostering Positive Thinking\nTidymodels\nWhat is it?\nIrregular Grid Searches\nRandom Grid Search\nSpace-Filling Designs\n\nusemodels\nstacks\nEnsemble Models\nstacks Grammar\n\n\n\nI attended several sessions throughout rstudio::global and below are my favorite reminders, takeaways, and new concepts that I learned. You‚Äôll gain the most from this article if you are comfortable with predictive analytics with R and have been exposed to the tidyverse and tidymodels collections of packages. If you missed the conference, all of the presentations and slides will be made available on RStudio‚Äôs website soon. Some philosophical, some technical.\nFostering Positive Thinking\nThe opening keynote presentation with RStudio‚Äôs Chief Data Scientist, Hadley Wickham, was super refreshing and very encouraging to hear. In short, Wickham encouraged analysts and data scientists to change the way you approach challenges when programming or maintaining code.\nBelow are several reminders that Wickham discussed‚Ä¶ If you‚Äôre your harshest critic, revisit these. I know that I will:\nAutomatic Thoughts\nBalanced Alternatives\nI must be a moron if I can‚Äôt perform this ‚Äúsimple‚Äù task.\nThis isn‚Äôt something I do very often so it‚Äôs unreasonable to expect that I‚Äôd automatically be an expert at it.\nThe documentation is useless and doesn‚Äôt help me at all.\nIt‚Äôs not possible to document every possible existing situation, but I can still read the docs and learn something.\nThis was a total waste of time. I‚Äôll never get those four hours back again.\nMaybe I didn‚Äôt succeed in my original goal, but I made some progress, and I gained valuable insights for the next time that I try.\nTidymodels\nWhat is it?\nComing from a converted caret-ista, it wasn‚Äôt an easy transition to the full tidymodels framework. I liked the way caret felt, and I did almost everything with it from resampling, modeling, tuning, to model evaluation. Enter tidymodels, a framework that feels like a deconstructed caret - Each major caret feature has been enhanced and wrapped into its own package. I‚Äôve been using this framework for roughly one year and the rate at which this ecosystem has evolved has been impressive.\ntidymodels is a collection of tidy-friendly packages written with a consistent vocabulary for reproducible machine learning workflows. Its core packages include the following:\nrsample: provides infrastructure for efficient data splitting and resampling\nparsnip: a tidy, unified interface to models that can be used to try a range of models without getting bogged down in the syntactical minutiae of the underlying packages\nrecipes: a tidy interface to data pre-processing tools for feature engineering\nworkflows: expands the traditional model-only recipe to a much more holistic blueprint for pre-processing, modeling, post-processing, and evaluation\ntune: helps you optimize the hyperparameters of your model and pre-processing steps\nyardstick: measures the effectiveness of models using performance metrics\nbroom: converts the information in common statistical R objects into user-friendly, predictable formats\ndials: creates and manages tuning parameters and parameter grids\nusemodels: a helpful way to quickly create code snippets to fit models using the tidymodels framework\nAt the time of writing this, usemodels is still in very early development (version 0.1.0.). This package follows the usethis philosophy of automating repetitive project development tasks but with a focus on modeling. usemodels helped me fully transition from the caret meta-engine to parsnip + tune + dials + more of the tidymodels framework.\n\nstacks: similar to usemodels, this is an early-stage package for model stacking, an ensembling method that takes the outputs of many models and combines them to generate a new model that generates predictions informed by each of its members\nIrregular Grid Searches\nIrregular grid searches help you tune your models by cycling through randomized hyperparameters in an effort to yield the best (or at least better) performance. I‚Äôve been using the dials package to create Latin hypercubes which is explained below, highlighting the benefits of this space-filling design vs a purely randomized irregular grid search. Max Kuhn‚Äôs ‚ÄúNew in tidymodels‚Äù rstudio::global event provided me with a fresh reminder about why these methods are beneficial and how they actually optimize performance.\nThe documentation below was copied almost entirely from Chapter 13 of Max Kuhn and Julia Silge‚Äôs Tidy Modeling with R (‚ÄúTMWR‚Äù). I‚Äôve deleted certain bits and added minimal helper comments in brackets. If you find yourself statistically programming, bootstrapping / resampling, improving model performance through grid searches, maybe even ensemble stacking, I‚Äôd highly recommend that you bookmark the TMWR book.\nRandom Grid Search\nThere a several options for creating non-regular grids. The first is to use random sampling across the range of parameters. The grid_random() function [of the dials package] generates independent uniform random numbers across the parameter ranges. If the parameter object has an associated transformation (such as we have for penalty), the random numbers are generated on the transformed scale. For example:\n\n\nset.seed(10)\nmlp_param %>% \n  grid_random(size = 1000) %>% # 'size' is the number of combinations\n  summary()\n#>   hidden_units      penalty           epochs    \n#>  Min.   : 1.00   Min.   :0.0000   Min.   :  10  \n#>  1st Qu.: 3.00   1st Qu.:0.0000   1st Qu.: 259  \n#>  Median : 6.00   Median :0.0000   Median : 480  \n#>  Mean   : 5.58   Mean   :0.0432   Mean   : 496  \n#>  3rd Qu.: 8.00   3rd Qu.:0.0050   3rd Qu.: 738  \n#>  Max.   :10.00   Max.   :0.9932   Max.   :1000\n\n\n\nFor penalty, the random numbers are uniform on the log (base 10) scale but the values in the grid are in the natural units.\nThe issue with random grids is that, with small-to-medium grids, random values can result in overlapping parameter combinations. Also, the random grid needs to cover the whole parameter space but the likelihood of good coverage increases with the number of grid values. Even for a sample of 15 candidate points, this plot shows some overlap between points for our example multilayer perceptron:\n\n\nlibrary(ggforce)\nset.seed(200)\nmlp_param %>% \n  # The 'original = FALSE' option keeps penalty in log10 units\n  grid_random(size = 15, original = FALSE) %>% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Random design with 15 candidates\")\n\n\n\nRandom grid search using dials::grid_random()Space-Filling Designs\nA much better approach is to use a set of experimental designs called space-filling designs. While different design methods have slightly different goals, they generally find a configuration of points that cover the parameter space with the smallest chance of overlapping or redundant values. See Santner et al.¬†(2003) for an overview of space-filling designs.\nThe dials package contains functions for Latin hypercube and maximum entropy designs. As with dials::grid_random(), the primary inputs are the number of parameter combinations and a parameter object. Let‚Äôs compare the above random design with a Latin hypercube design for 15 candidate parameter values.\n\n\nset.seed(200)\nmlp_param %>% \n  grid_latin_hypercube(size = 15, original = FALSE) %>% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Latin Hypercube design with 15 candidates\")\n\n\n\nLatin hypercube designed using the dials::grid_latin_hypercube() helperWhile not perfect, this design spaces the points further away from one another.\nSpace-filling designs can be very effective at representing the parameter space. The default design used by the tune package is the maximum entropy design. These tend to produce grids that cover the candidate space well and drastically increase the chances of finding good results.\nTo learn more about advanced iterative search methods such as Bayesian optimization and simulated annealing, please visit Chapter 14 of TMWR. Max Kuhn covered these advanced techniques during his tidymodels rstudio::global event and I haven‚Äôt had a chance to try these iterative search methods.\nusemodels\nIn the spirit of sharing useful tips, tricks, and packages, I‚Äôd like to introduce you to Max Kuhn‚Äôs usemodels package. If you‚Äôre on the fence about adopting the tidymodels framework, this package is definitely for you. Even for experienced users, usemodels is an excellent way to quickly generate boilerplate code snippets that are algorithm-specific. This package is not yet on CRAN, so install it using the following:\n\n\n# Installing usemodels from its GitHub repo\ndevtools::install_github(\"tidymodels/usemodels\")\n\n\n\nGiven a simple formula and a data set, the use_* functions can create code that is appropriate for the data (given the model). [Note: The model formula will be in the form of y ~ a + b + c or y ~ . if you plan on including all available variables in your model.]\nFor example, using the palmerpenguins data with a glmnet model:\n\n\nlibrary(usemodels)\nlibrary(palmerpenguins)\ndata(penguins)\nuse_glmnet(body_mass_g ~ ., data = penguins)\n\n# NOTE: The below will be printed in your console with your model recipe and tailored\n# with the required pre-processing steps given your algorithm of choice (in this case,\n# glmnet). The `usemodels` output also provides the code structure for a reproducible \n# workflow, made possible with the `workflows` package. Should you choose to tune\n# your model, `usemodels` also provides code snippets for producing a grid of parameter\n# combinations to use with your hyperparameter tuning grid search.\n\nglmnet_recipe <- \n  recipe(formula = body_mass_g ~ ., data = penguins) %>% \n  step_novel(all_nominal(), -all_outcomes()) %>% \n  step_dummy(all_nominal(), -all_outcomes()) %>% \n  step_zv(all_predictors()) %>% \n  step_normalize(all_predictors(), -all_nominal()) \n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20), mixture = c(0.05, \n    0.2, 0.4, 0.6, 0.8, 1)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, resamples = stop(\"add your rsample object\"), grid = glmnet_grid)\n\n\n\nAs of today‚Äôs date, this package includes templates available with the following code: use_cubist, use_earth, use_glmnet, use_kknn, use_ranger, and use_xgboost\nstacks\nEnsemble Models\nSimilar to usemodels, this is a very early-stage package that you should start following. There are already several AI tools for producing ‚Äúmodel stacks‚Äù or ‚Äúensemble models‚Äù such as H2O.ai, DataRobot, and a few others, however, stacks is the first purpose-built for use with tidymodels. The following narrative is copied almost directly from the stacks website.\nstacks is an R package for model stacking that aligns with the tidymodels. Model stacking is an ensembling method that takes the outputs of many models and combines them to generate a new model - referred to as an ensemble in this package - that generates predictions informed by each of its members.\nThe process goes something like this:\nDefine candidate ensemble members using functionality from rsample, parsnip, workflows, recipes, and tune\nInitialize a data_stack object with stacks()\nIteratively add candidate ensemble members to the data_stack with add_candidates()\nEvaluate how to combine their predictions with blend_predictions()\nFit candidate ensemble members with non-zero stacking coefficients with fit_members()\nPredict on new data with predict()\nstacks Grammar\nAt the highest level, ensembles are formed from model definitions. In this package, model definitions are an instance of a minimal workflow, containing a model specification (as defined in the parsnip package) and, optionally, a preprocessor (as defined in the recipes package). Model definitions specify the form of candidate ensemble members.\n\nTo be used in the same ensemble, each of these model definitions must share the same resample. This rsample rset object, when paired with the model definitions, can be used to generate the tuning/fitting results objects for the candidate ensemble members with tune.\n\nCandidate members first come together in a data_stack object through the add_candidates() function. Principally, these objects are just tibbles, where the first column gives the true outcome in the assessment set (the portion of the training set used for model validation), and the remaining columns give the predictions from each candidate ensemble member. (When the outcome is numeric, there‚Äôs only one column per candidate ensemble member. Classification requires as many columns per candidate as there are levels in the outcome variable.) They also bring along a few extra attributes to keep track of model definitions.\n\nThen, the data stack can be evaluated using blend_predictions() to determine to how best to combine the outputs from each of the candidate members. In the stacking literature, this process is commonly called meta-learning.\nThe outputs of each member are likely highly correlated. Thus, depending on the degree of regularization you choose, the coefficients for the inputs of (possibly) many of the members will zero out‚Äîtheir predictions will have no influence on the final output, and those terms will thus be thrown out.\n\nThese stacking coefficients determine which candidate ensemble members will become ensemble members. Candidates with non-zero stacking coefficients are then fitted on the whole training set, altogether making up a model_stack object.\n\nThis model stack object, outputted from fit_members(), is ready to predict on new data! The trained ensemble members are often referred to as base models in the stacking literature. To learn more about how to use stacks, check out the following excellent vignettes from the tidymodels team:\nGetting Started with stacks\nClassification Models with stacks\nSources:\nrstudio::global(2021)\nTidy Modeling with R\ntidymodels\nusemodels\nstacks\n\n\n\n",
    "preview": "posts/2021-01-24-rstudio-global/images/rstudio-global-2021.jpg",
    "last_modified": "2021-02-06T20:37:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-19-rsthemes/",
    "title": "rsthemes: Customizing your RStudio IDE",
    "description": "Change up your editor theme with {rsthemes},  a collection of themes to freshen up the  RStudio IDE aesthetics.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2021-01-19",
    "categories": [
      "RStudio",
      "Packages",
      "R",
      "Quick Tips"
    ],
    "contents": "\nFeel like freshening up your RStudio IDE? Check out the rsthemes package by Garrick Aden-Buie to explore and apply different themes. For the last year or so, I‚Äôve been enjoying the ‚Äúbase16 Ashes‚Äù theme at work and home. I prefer the ‚ÄúOne Dark‚Äù theme on Atom / Juno for my Julia setup, But I‚Äôm starting to feel like I should use ‚ÄúOne Dark‚Äù across the board. ü§ì\nThis package is not yet on CRAN, so you‚Äôll have to install via devtools::install_github(). See installation and usage code below, pulled from the rsthemes GitHub:\n\n\n## INSTALLATION\n# Instructions assume devtools is installed)\ndevtools::install_github(\"gadenbuie/rsthemes\")\n\n# Install custom themes + additional set of base16-based themes\nrsthemes::install_rsthemes(include_base16 = TRUE)\n\n## USAGE\n# list installed themes\nrsthemes::list_rsthemes()\n\n# Try all themes\nrsthemes::try_rsthemes()\n\n# Try just the light, dark, or base16 themes, e.g.\nrsthemes::try_rsthemes(\"light\")\n\n# Use a theme\nrstudioapi::applyTheme(\"One Dark {rsthemes}\")\n\n\n\nSource:\nrsthemes on GitHub\nbase16 themes\n\n\n\n",
    "preview": "posts/2021-01-19-rsthemes/images/rsthemes.gif",
    "last_modified": "2021-01-28T21:23:51-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-31-Raspberry-Pi/",
    "title": "Preparing for 2021 Goals with a Raspberry Pi",
    "description": "I have been planning my 2021 goals and the Raspberry Pi 4 will help me kill a few birds with one stone.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "Julia",
      "Linux",
      "Raspberry Pi"
    ],
    "contents": "\nKilling a few birds with one stone and getting ready for 2021‚Ä¶ 1) Getting better at Linux / shell commands,  2) Learning Julia (see my Resources tab for some great Julia starter links!),  3) Doing it all on a Raspberry Pi üòú Happy holidays and Happy New Years to all!\nBelow are some pics of my current progress.\nRaspberry Pi 4, Model B, 8GB RAM, & Super Cute Tiny Heat SinksEnclosing the Pi 4 in a CanaKit case + fanRaspberry Pi OS, a Debian distro for the PiJupyter Lab server on the Pi set up w/ Python, R, and Julia\n\n\n",
    "preview": "posts/2020-12-31-Raspberry-Pi/images/Pi-banner.png",
    "last_modified": "2021-01-22T20:57:54-08:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 630
  },
  {
    "path": "posts/2020-12-19-R-Package-One-Hour-Tutorial/",
    "title": "One-Hour R Package Development Tutorial by Shannon Pileggi, PhD",
    "description": "I followed this tutorial and created a package successfully. It took 45 minutes. Follow, this, guide, my fellow R friends!",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-12-19",
    "categories": [
      "R",
      "Packages",
      "Tutorial"
    ],
    "contents": "\nPackage development should be on every R programmer‚Äôs radar in 2021, especially with all the free information available online! Here‚Äôs an excellent, concise, ultra-bookmark-worthy tutorial by Shannon Pileggi, PhD, titled ‚ÄúYour first R package in 1 hour‚Äù. Folks, she wasn‚Äôt lying either. I followed her tutorial and built an R package in roughly 45 minutes (note: I assume you have experience with the R language and understand the basics of devtools, usethis, roxygen2, and rmarkdown).\nUsing functions from just the devtools and usethis packages, the whole process is much more streamlined than what I would have thought. Check out the rest of Shannon‚Äôs Piping Hot Data website as well‚Ä¶ It‚Äôs a great resource for R programmers and researchers, and she built it with the newly reintroduced distill package, just like this site. :-)\nSource: * Shannon‚Äôs Piping Hot Data ‚ÄúYour first R package in 1 hour‚Äù post * Shannon‚Äôs LinkedIn\n\n\n\n",
    "preview": "posts/2020-12-19-R-Package-One-Hour-Tutorial/images/package_workshop.jpg",
    "last_modified": "2021-01-20T22:31:44-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-04-dbplyr-2.0.0/",
    "title": "What's New in dbplyr 2.0.0",
    "description": "dbplyr, a database backend for dplyr, just released  v2.0.0 today... Awesome stuff here!",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-11-04",
    "categories": [
      "RStudio",
      "tidyverse",
      "Packages",
      "R",
      "SQL"
    ],
    "contents": "\ndbplyr 2.0.0 comes loaded with improvements including the translation of dplyr‚Äôs new across() function. If you aren‚Äôt familiar with this library, dbplyr translates your dplyr syntax to SQL. I regularly connect to enterprise databases using R and have almost completely transitioned from SQL. Yes, SQL is awesome. Yes, using dplyr is a lot more fun!\n\nSource: * tidyverse blog‚Äôs dbplyr 2.0.0 Official Announcement\n\n\n\n",
    "preview": "posts/2020-11-04-dbplyr-2.0.0/images/dbplyr_what_is_new.jpg_large",
    "last_modified": "2021-01-20T22:16:49-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-13-R-Package-Detailed-Tutorial/",
    "title": "Detailed R Package Development Tutorial from Method Bites",
    "description": "This is one of the best, most detailed, how-to  guides for developing your own R package from A-to-Z",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-09-13",
    "categories": [
      "Packages",
      "R",
      "Tutorial"
    ],
    "contents": "\nI‚Äôll keep this short since all credit should go towards the amazing Cosima Meyer and Dennis Hammerschmidton‚Ä¶ They compressed some of the most valuable steps for creating and deploying a package. This duo‚Äôs work was published for the MZES Social Science Data Lab. If you want a detailed and easy-to-follow how-to for R package development, bookmark this step-by-step guide!\n\nSource: * Methods Bites: How to write your own R package and publish it on CRAN\n\n\n\n",
    "preview": "posts/2020-09-13-R-Package-Detailed-Tutorial/images/methods_bites.png",
    "last_modified": "2021-01-20T22:49:31-08:00",
    "input_file": {},
    "preview_width": 1037,
    "preview_height": 1200
  },
  {
    "path": "posts/2020-07-03-Shiny-Voice-Input/",
    "title": "Shiny Voice-Activated input",
    "description": "Want to make your Shiny apps voice-interactive? Now it's possible.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-07-03",
    "categories": [
      "Shiny",
      "R",
      "Packages"
    ],
    "contents": "\nHey R Shiny users, want to make your Shiny apps voice-interactive? For example, ‚ÄúClick on Product Toy-2A and then drill-down by Region.‚Äù Ummm, yes please. Pretty neat stuff made possible with the heyshiny package. ‚ÄúThe heyshiny package provides a new Shiny input, the speechInput(). This new input allows your Shiny app to listen to the microphone, recognize the speech, and return it as text.‚Äù\nThe main caveat with this package is that it is based on the annyang JavaScript library and therefore requires that 1) you‚Äôre online and 2) using a browser that supports speech recognition (I tried this on Chrome and works fine). It‚Äôs not on CRAN but check out the GitHub page, below. Here‚Äôs an example if you‚Äôre getting set up, pulled from their GitHub repo:\n\nSource: * heyshiny on GitHub\n\n\n\n",
    "preview": "posts/2020-07-03-Shiny-Voice-Input/images/shiny_logo.png",
    "last_modified": "2021-01-20T22:46:40-08:00",
    "input_file": {},
    "preview_width": 860,
    "preview_height": 414
  },
  {
    "path": "posts/2020-06-20-dplyr-1.0.0/",
    "title": "Testing dplyr 1.0.0",
    "description": "Finally getting around to trying out dplyr 1.0.0... Love it!",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-06-20",
    "categories": [
      "Packages",
      "R",
      "tidyverse"
    ],
    "contents": "\nI had read about the new across() function hype and now I get it‚Ä¶ No more mutate_at() or summarise_at() with the cryptic list-lambdas. As the dplyr documentation states, ‚Äúacross() supersedes the family of‚Äùscoped variants\" like summarise_at(), summarise_if(), and summarise_all().\"\nBelow is an example highlighting the new across() syntax meant to be used within a mutate() function. I tried this on a few columns of the mtcars data set with dplyr‚Äôs case_when() a la SQL CASE_WHEN (as opposed to nesting multiple if_else() statements to the point of confusion!). The dplyr changes are subtle but will definitely streamline my data wrangling.\ndplyr 1.0.0‚Äôs new across() functionSource: * dplyr, a core tidyverse package\n\n\n\n",
    "preview": "posts/2020-06-20-dplyr-1.0.0/images/dplyr.jpeg",
    "last_modified": "2021-01-22T20:59:09-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-19-Police-Killings/",
    "title": "Lying with Statistics",
    "description": "It's quite easy to manipulate raw data in a manner that \"proves\" your point. For the sake of exploring this topic further, I'll analyze police killing data and present it in three different ways.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-04-19",
    "categories": [
      "Statistics",
      "ggplot2",
      "Visualizations",
      "R"
    ],
    "contents": "\nLets explore four plots and see how we can #LieWithStatistics‚Ä¶\nPlot 1: Police killings by date, by race General observation: Police kill more white people than black people\nPlot 2: Police killing boxplot showing murder rates, by race, by police department General explanation and takeaway: The dots on each boxplot show the statistical outliers, box plot lines extend out to the ‚Äúmin‚Äù and ‚Äúmax‚Äù, and the box lines (from bottom to top of each box) represent the first quartile (25th percentile), median (50th percentile), and third quartile (75th percentile)\nPlot 3: Police killing boxplot, now log-transforming the murder rates to more easily identify statistical differences, by race General explanation and takeaway: Log-transforming data points for visualization or modeling purposes is a technique by which you can smooth observed data making it more robust (or resistant) to outliers. I effectively re-wrote the murder rates to show exponential relativity. Important caveat: Are Native Americans more likely to die by police than other races? Sure looks like it‚Ä¶ but see Plot 4 for more thoughts\nPlot 4: Police killing boxplot, now log-transforming the murder rates using a log base 10 (easier interpretability) and ‚Äúfixing‚Äù the Native American data points causing a misleading assumption in Plot 3, i.e., Native American death rates appeared much higher than others in Plot 3 given the fact that log(0) = 1. General takeaway: There were such few Native American data points that log-transforming all of the zeroes was unintentionally bastardizing the analysis. It would appear black people are almost an order of magnitude more likely to be killed by police than white people.\nI do not seek to answer questions of ‚Äúwhy‚Äù systemic injustice exists in the US, but I wanted to analyze police killing data and share these dialectical investigations.\nSource: * Samuel Sinyangwe & the Mapping Police Violence team\n\n\n\n",
    "preview": "posts/2020-04-19-Police-Killings/images/Police_Killings_1.jpeg",
    "last_modified": "2021-01-28T21:19:35-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-04-05-DnD-Adventures/",
    "title": "D&D Adventures + blogdown",
    "description": "It's an incredible time to be creative! I've recently been re-learning Dungeons & Dragons with a group of  friends and having so much fun in the process.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-04-05",
    "categories": [
      "Gaming",
      "blogdown"
    ],
    "contents": "\nWizards of the Coast have developed such an incredible tabletop, fantasy, RPG with the 5th edition of D&D‚Ä¶ I wish I would have re-discovered this game years back. For context, I haven‚Äôt touched D&D since 1997!\nIf you know of good resources for newbies and new D&D Dungeon Masters (‚ÄúDMs‚Äù), I‚Äôd love to know about your favorite resources. For data science folks out there, this site was built with R using blogdown, Hugo themes, and deployed for free using GitHub and Netlify.\nD&D dragon looking for troubleReference:  * Javier‚Äôs D&D Adventures\n\n\n\n",
    "preview": "posts/2020-04-05-DnD-Adventures/images/Dnd.png",
    "last_modified": "2021-01-20T13:09:33-08:00",
    "input_file": {},
    "preview_width": 1446,
    "preview_height": 612
  },
  {
    "path": "posts/2020-02-20-Themis/",
    "title": "themis: Extra Steps for tidymodels + recipes",
    "description": "themis contain extra steps for the recipes package for dealing with unbalanced data. The name themis is that of the ancient Greek goddess who is typically depicted with a balance.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-02-20",
    "categories": [
      "R",
      "tidymodels",
      "Packages"
    ],
    "contents": "\nWorking with unbalanced data sets? Remember that accuracy, alone, is not the best performance metric (especially when dealing with unbalanced data). Instead, place more importance on Cohen‚Äôs kappa coefficient, F1 harmonic mean, or focus on improving your model‚Äôs specificity or sensitivity, etc.\nI‚Äôve been transitioning a lot of my workflows to the tidymodels framework and I am super excited about the future of tidymodels (recipes + parsnip + dials + tune + workflow + more üò≠‚úäüôå). If you‚Äôre using recipes often like me, a new library called {themis}, by Emil Hvitfeldt expands the {recipes} pre-processing steps for working with unbalanced data sets (it adds functionality for under- and hybrid-sampling techniques). I love me some smote, and now I can incorporate this sampling technique into my recipes with themis::step_smote()!\n# Installation\ninstall.packages(\"themis\")\n\n# Example workflow\nlibrary(recipes)\nlibrary(modeldata)\nlibrary(themis)\n\ndata(okc)\n\nsort(table(okc$Class, useNA = \"always\"))\n#> \n#>  <NA>  stem other \n#>     0  9539 50316\n\nds_rec <- recipe(Class ~ age + height, data = okc) %>%\n  step_meanimpute(all_predictors()) %>%\n  step_smote(Class) %>%\n  prep()\n\nsort(table(bake(ds_rec, new_data = NULL)$Class, useNA = \"always\"))\n#> \n#>  <NA>  stem other \n#>     0 50316 50316\nSource: * themis * themis on GitHub\n\n\n\n",
    "preview": "posts/2020-02-20-Themis/images/themis.png",
    "last_modified": "2021-01-19T21:33:28-08:00",
    "input_file": {},
    "preview_width": 2828,
    "preview_height": 2472
  },
  {
    "path": "posts/2020-02-08-R-write-table/",
    "title": "Copy R Objects to Clipboard",
    "description": "I was today years old when I learned that you could easily copy/paste R objects to your clipboard.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-02-08",
    "categories": [
      "R",
      "Quick Tips"
    ],
    "contents": "\nYou learn something new every day, even basics! For R users on Windows (I‚Äôm assuming the same holds true for Mac and Linux users), do you ever find yourself needing to quickly export a dataframe or tibble to Excel or a text editor? I recently discovered base R‚Äôs write.table function and have used it much more often than I anticipated.\nSee below syntax as an example:\nbase R‚Äôs write.table\n\n\n",
    "preview": "posts/2020-02-08-R-write-table/images/carbon.png",
    "last_modified": "2021-01-19T21:05:27-08:00",
    "input_file": {},
    "preview_width": 2692,
    "preview_height": 1968
  },
  {
    "path": "posts/2020-02-04-Hadley-Wickham-on-SDS-Podcast/",
    "title": "Hadley Wickham on SuperDataScience Podcast",
    "description": "My eyes were opened to the world of analytics and data science in part through Kirill Eremenko and his amazing SuperDataScience podcast.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-02-04",
    "categories": [
      "RStudio",
      "R",
      "Python",
      "SuperDataScience"
    ],
    "contents": "\nHands down, he was the inspiration for me starting my own podcast and I still listen to SDS weekly. Being a massive fan of RStudio, I‚Äôm super excited about this most recent SDS episode featuring Hadley Wickham, RStudio‚Äôs Chief Data Scientist and creator of ggplot2, dplyr, author, professor‚Ä¶ he‚Äôs a research and data science genius!.\nThis episode is a great listen as he explores RStudio Python and R integration and the future of data science.\n\nSource:\nSuperDataScience ep.337 with Hadley Wickham\n\n\n\n",
    "preview": "posts/2020-02-04-Hadley-Wickham-on-SDS-Podcast/images/SDS_Wickham.jpg",
    "last_modified": "2021-01-28T21:20:40-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-01-k-fold-cross-validation/",
    "title": "Resampling with k-fold Cross Validation",
    "description": "Click for a high-level recap of k-fold cross  validation as a resampling method.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-02-01",
    "categories": [
      "Statistics",
      "Machine Learning",
      "R"
    ],
    "contents": "\nk-fold ‚Äúrandomly divides the training data into k groups (or folds) of approximate size [with the model being] fit on k-1 folds and the remaining fold used to compute model performance.‚Äù I use k=5 or k=10 given that this is typical industry practice without really questioning ‚Äúwhy?‚Äù\n\nIn reading through Hands-On Machine Learning with R by Brad Boehmke, Ph.D.¬†and Brandon Greenwell, I was surprised to learn that studies have shown that k=10 performs similarly to leave-one-out cross validation where k=n.\nWithout realizing it, sometimes I get carried away optimizing code and drifting from statistics and the core ‚Äúscience‚Äù in data science‚Ä¶ k-fold CV is a technique used by many and is agnostic to your statistical programming language of choice but if you‚Äôre an #R user, I can‚Äôt recommend this book enough (free in full online, link below)!\nSource:\nHands-On Machine Learning with R Chapter 2.4 Resampling Methods\n\n\n\n",
    "preview": "posts/2020-02-01-k-fold-cross-validation/images/kfold.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-22-Systematically-Improving-Coffee/",
    "title": "An Algorithm for Better Espresso",
    "description": "Coffee lovers... You might find this study  fascinating. Finally, a model for the perfect  cup of espresso!.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-01-22",
    "categories": [
      "Statistics",
      "Coffee"
    ],
    "contents": "\nI upgraded my espresso machine over the holidays and have been pleasantly surprised by the need to recalibrate my machine‚Äôs settings with every new batch of coffee (and of course, fresh roasted is best). Glad to see some math support my observations. :-)\n\nSource:\nSystematically Improving Espresso: Insights from Mathematical Modeling and Experiment\n\n\n\n",
    "preview": "posts/2020-01-22-Systematically-Improving-Coffee/images/espresso_extraction.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-20-Puerto-Rico-Earthquakes/",
    "title": "Analyzing Earthquakes in Puerto Rico",
    "description": "Puerto Rico has been experiencing atypical  seismic activity since November 2019.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-01-20",
    "categories": [
      "Statistics",
      "ggplot2",
      "Earthquakes"
    ],
    "contents": "\nMy mother‚Äôs hometown, Ponce, has been hit badly by the recent earthquakes. A cousin asked me to ‚Äúplay with some data‚Äù and turns out that the USGS maintains great data on seismic activity in the Caribbean. I focused on coordinates close to PR to produce the attached visual. I‚Äôll revisit this visualization in a few months when more data becomes available!\n\nLinks:\nU.S. Geologival Survey: https://www.usgs.gov/\n\n\n\n",
    "preview": "posts/2020-01-20-Puerto-Rico-Earthquakes/images/Earthquakes_PR.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-18-Create-R-Packages/",
    "title": "Create Your Own R Package",
    "description": "If you are interested in developing your own R packages, this thorough A-to-Z tutorial resource is one to bookmark.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-01-18",
    "categories": [
      "R",
      "Packages",
      "Development"
    ],
    "contents": "\n‚ÄúThis book espouses our philosophy of package development: anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions.‚Äù Great quote by Hadley Wickham and Jenny Bryan in their book ‚ÄúR Packages: Organize, Test, Document and Share Your Code,‚Äù also available online (link below).\nA coworker sent me this link and I‚Äôm super excited to start developing some packages. Great timing as one of my 2020 resolutions is to write my own package(s)!\n\nLinks:\nR Packages by Hadley Wickham and Jenny Bryan\n\n\n\n",
    "preview": "posts/2020-01-18-Create-R-Packages/images/R_Packages.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-09-Writing-to-Excel-from-R/",
    "title": "Writing to Excel from R",
    "description": "Spending your days R but working closely with analysts  and leadership that live in Excel? Get familiar with openxlsx.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-01-09",
    "categories": [
      "R",
      "Packages",
      "Quick Tips"
    ],
    "contents": "\nopenxlsx is an awesome package for writing multiple datasets to multiple sheets of an Excel workbook, and allows for robust styling and cell formatting. I found a tutorial on RPubs written by Ezekiel Adebayo Ogundepo covering this package that I often use at work. Definitely bookmark worthy.\nHere is a high-level code example:\nlibrary(openxlsx)\n\n# Name the Excel worksheets where you want to export data to\nlist_of_datasets <- list(\"Overall Summary\" = df_1, \"LOB Summary\" = df_2, \"Details\" = df_3)\n\n# Save the Excel workbook to your wd with your worksheet list and name the Excel file\nwrite.xlsx(list_of_datasets, \"Excel_Workbook_2017.01.20.xlsx\")\nLinks:\nEzekiel‚Äôs openxlsx tutorial on RPubs\nopenxlsx repository on GitHub\n\n\n\n",
    "preview": "posts/2020-01-09-Writing-to-Excel-from-R/images/openxlsx.png",
    "last_modified": "2021-01-28T21:22:05-08:00",
    "input_file": {},
    "preview_width": 3636,
    "preview_height": 1248
  },
  {
    "path": "posts/2020-01-04-CCPA/",
    "title": "California Consumer Privacy Act",
    "description": "The California Consumer Privacy Act is set to go into effect on January 1, 2020. The CCPA, similar in nature to GDPR, will provide California residents with new consumer data rights.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-01-04",
    "categories": [
      "Privacy"
    ],
    "contents": "\n\nThe California Consumer Privacy Act (‚ÄúCCPA‚Äù) data rights include, but are not limited to, the following:\nthe right to know,\nthe right to delete,\nthe right to opt-out of personal data sale, and\nthe right to non-discrimination when a consumer exercises a privacy right under CCPA.\nCCPA will only apply to businesses that meet one or more of these three criteria:\nhave annual revenues greater than $25 million USD,\nbusinesses that buy / receive / sell personal information of 50k+ consumers, or\nbusinesses that derive 50% or more of annual revenues from selling consumers‚Äô personal information\nCCPA exempt businesses include HIPAA-compliant health insurers and providers, certain financial institutions, and credit reporting agencies.\nThe expected fine per unintentional and intentional violation is $2,500 and $7,500, respectively. Fines make sense, but I hope that the California Department of Justice can establish clear reporting and audit requirements to enforce these new regulations.\nSource:\nCNET‚Äôs oveview of CCPA\n\n\n\n",
    "preview": "posts/2020-01-04-CCPA/images/CCPA.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-01-Holidays/",
    "title": "Holiday Expectations vs Reality",
    "description": "Holiday expectations...",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2020-01-01",
    "categories": [
      "Gaming"
    ],
    "contents": "\n Holiday expectation: Going to finish a few books, an online class that‚Äôs been lingering, research‚Ä¶\nHoliday reality: Make a million espressos, eat loads of chocolate and cake, play more ‚ÄòThe Legend of Zelda: Link‚Äôs Awakening‚Äô than I thought was humanly possible‚Ä¶\nLife is good folks, Merry Christmas and I hope that you got to enjoy some time with family and friends this holiday season!\n\n\n\n",
    "preview": "posts/2020-01-01-Holidays/images/Zelda.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-31-Ubuntu-Update/",
    "title": "Ubuntu Update to 19.10",
    "description": "Things not to do with Linux, learned the hard way, as we approach the end of 2019...",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-12-31",
    "categories": [
      "Linux",
      "Ubuntu"
    ],
    "contents": "\n\nUpdate Ubuntu 19.04 to 19.10 on dual-booted systems. Not sure exactly where things broke down for me, but as of right now, #Ubuntu seems inaccessible. Not too bummed about it as everything is backed up, but if you‚Äôve recently had the same heartburn and found a solution, please send me a message!\n\n\n\n",
    "preview": "posts/2019-12-31-Ubuntu-Update/images/Ubuntu.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/",
    "title": "Book Rec: Hands-On Machine Learning with R",
    "description": "Digging into Hands-On Machine Learning with R by Brad Boehmke, PhD, and Brandon Greenwell.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-12-17",
    "categories": [
      "Machine Learning",
      "R",
      "Packages"
    ],
    "contents": "\n\nI am super excited to finally dig into Hands-On Machine Learning with R by Brad Boehmke, Ph.D. and Brandon Greenwell.\nThere are a ton of solid ML books on the market for Python and R users but I‚Äôve struggled with developing a best practice ML workflow to make maintainable code and to enhance my sampling, evaluation, and iteration process. As an R user, I‚Äôm glad to have this hands-on approach resource to improve my use of the ML stack within R.\nThe full text and code samples are available online, for free: https://lnkd.in/gvmXY3W\n\n\n\n",
    "preview": "posts/2019-12-17-Hands-On-Machine-Learning-with-R/images/HOMLR.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-15-Java-Joy/",
    "title": "Spreading Joy with Java Joy",
    "description": "Bay Area friends... Next time your organization needs coffee for an event, quarterly meeting, all-hands, or lunch and learn, reach out to Java Joy!",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-12-15",
    "categories": [
      "Social Good"
    ],
    "contents": "\n One of my childhood friends from Atlanta, Chad Lindsey, has helped this amazing company come to SF and they‚Äôd be grateful for your business. Check Java Joy‚Äôs website or connect with them on LinkedIn (links below) to learn more.\nJava Joy‚Äôs mission is to empower people of all abilities to transform others by spreading their unmatched joy. Their vision is to become the largest employer and best place to work in the U.S. for adults with disabilities. Their vehicle is a mobile coffee cart and a passion for joy.\nInteresting fact that I learned from their website: 75% of adults with disabilities CAN and WANT to work, but only 35% are engaged in meaningful employment.\nLinks:\nFind more info about Java Joy here: https://java-joy.org/\nJava Joy on Linked\n\n\n\n",
    "preview": "posts/2019-12-15-Java-Joy/images/Java_Joy.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-04-Finland-Education-System/",
    "title": "Education Reform and Lessons from Finland",
    "description": "Quartz and Google's Avinash Kaushik share insights comparing national education systems, test performance, and time spent on learning..",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-12-04",
    "categories": [
      "Education"
    ],
    "contents": "\n Despite Finnish students spending the least time per week learning (and starting school later, and having much less homework), their students have some of the highest reading test scores.\nNot captured in this study, but I‚Äôd be curious to see how external factors impact these results. We have to assume American parents spend a lot less time helping their kids learn (imagine having parents that work 2 or 3 jobs to make ends meet). Or how does average classroom size differ from country to country? Or distance traveled from home to school? Or distance to local libraries? Either way, the Finnish are doing something right here.\nLinks:\nFinland has the most efficient education system in the world on Quartz\nGoogle‚Äôs Avinash Kaushik‚Äôs post on LinkedIn\n\n\n\n",
    "preview": "posts/2019-12-04-Finland-Education-System/images/Finland.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-18-Blogdown/",
    "title": "Building a Website with Blogdown",
    "description": "Learn how to build and deploy a website with R, blogdown, GitHub, Hugo, and Netlify.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-11-18",
    "categories": [
      "Development",
      "Packages",
      "R",
      "blogdown"
    ],
    "contents": "\nPablo Barajas, a recent graduate at UC Irvine and friend of Scatter Podcast, led a presentation on how to build a website using R, RStudio, and Blogdown.\nFor anyone wanting to fairly easily build an online portfolio or blog that looks professional, I highly recommend Blogdown. I used this process to build this website, as did Pablo for his personal website, and I can help if you have any questions. Here‚Äôs the process in short: RStudio site build with Blogdown -> Commit to GitHub -> Apply Hugo theme -> Netlify for CI/CD\nThis event was hosted by the Orange County R Users Group (‚ÄúOCRUG‚Äù) and UCI‚Äôs Merage Analytics Club (‚ÄúMAC‚Äù). If you‚Äôre in Orange County and looking to network and learn with local data science practitioners and students, I highly recommend attending OCRUG events, R-Ladies Irvine events, or public MAC events. Great learning opportunities here!\nLinks:\nPablo‚Äôs presentation on his Blogdown site\nBlogdown overview from RStudio\nOCRUG‚Äôs homepage\nR-Ladies Irvine chapter\nMerage Analytics Club website\n\n\n\n",
    "preview": "posts/2019-11-18-Blogdown/images/blogdown.jpg",
    "last_modified": "2021-01-28T21:17:55-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-11-R-Markown/",
    "title": "15 Tips for Making Better Use of R",
    "description": "If you are an R user not using R Markdown, these tricks could be helpful for you.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-11-11",
    "categories": [
      "R",
      "R Markdown"
    ],
    "contents": "\n\nIt‚Äôs such a great way to make reports come to life. I‚Äôve been using it more in the last month than I have in the last year and can‚Äôt believe how much more interactive I‚Äôve made my deliverables (and even my own data exploration). Really excited to see what new tricks I can learn from RStudio‚Äôs webinar on Friday, November 15!\nLinks:\nRStudio Webinar Signup on Eventbrite\n\n\n\n",
    "preview": "posts/2019-11-11-R-Markown/images/RStudio-Webinar.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-11-05-R-Shiny-Expert-Course/",
    "title": "Shiny Developer Expert Course",
    "description": "Matt Dancho and Business Science introduce a new R Shiny Expert Developer course.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-11-05",
    "categories": [
      "Shiny",
      "R"
    ],
    "contents": "\n\nI am super excited about the new R Shiny Developer course that Matt Dancho launched, especially since he is the next guest on Scatter Podcast.\nHe‚Äôs one of the best data science educators that I‚Äôve learned from and the deeper that I dive into my data science career, the more and more value I find from his Business Science courses, Learning Labs, his online tutorials (on the tidyverse, predictive analytics, data science best practices), his R packages‚Ä¶ The guy is a beast!\nIf you are familiar with Shiny but want to gain some expert level Shiny skills, this class is for you. Great job with everything that you‚Äôre doing Matt and I‚Äôm excited to release your episode in a few days!\nLinks:\nBusiness Science‚Äôs Expert Shiny Developer with AWS\n15% off Business Science 4-course R bundle for Scatter Podcast listeners\nScatter Podcast Episode 25 w/ Matt Dancho\n\n\n\n",
    "preview": "posts/2019-11-05-R-Shiny-Expert-Course/images/business-science.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-22-BBC-Visualization-Journey-with-R/",
    "title": "Visual & Data Journalism at BBC",
    "description": "The BBC News data science and visualization team published this great overview on their analytics and visualizations lessons learned over the course of 1.5 years.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-10-22",
    "categories": [
      "ggplot2",
      "R",
      "Visualizations"
    ],
    "contents": "\n\nGreat read, AND super neat to see the BBC team open-source their BBPLOT and R Cookbook to help streamline your visualization workflows, reduce manual repetition of code setup for a ggplot2 viz, and to educate you on better storytelling approaches with R.\nLinks:\n‚ÄúHow the BBC Visual and Data Journalism team works with graphics in R‚Äù by BBC Visual and Data Journalism\nBBC‚Äôs bbplot package on GitHub\nBBC‚Äôs R Cookbook\n\n\n\n",
    "preview": "posts/2019-10-22-BBC-Visualization-Journey-with-R/images/BBC.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-10-19-Information-is-Beautiful-Awards/",
    "title": "Information is Beautiful 2019 Awards",
    "description": "Visualization professionals present their best at the annual Information is Beautiful Awards show.",
    "author": [
      {
        "name": "Javier Orraca",
        "url": {}
      }
    ],
    "date": "2019-10-19",
    "categories": [
      "Visualizations"
    ],
    "contents": "\n\nA few weeks ago, Will Chase was a guest on Scatter Podcast to talk about his work as a researcher and data visualization professional.\nSuper exciting news for Will‚Ä¶ His visualizing earthquake risk project was shortlisted for the Information is Beautiful 2019 Awards (the Science & Technology category). Give it a look and if you like what you see, please vote for his amazing project!! Congrats Will!\nLinks:\nVote for Will‚Äôs project on Kantar‚Äôs Information is Beautiful 2019 Awards\nWill‚Äôs (new!) website: www.williamrchase.com\n\n\n\n",
    "preview": "posts/2019-10-19-Information-is-Beautiful-Awards/images/EarthquakeRisk.jpg",
    "last_modified": "2021-01-18T21:31:43-08:00",
    "input_file": {}
  }
]
